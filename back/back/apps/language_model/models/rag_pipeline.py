from django.contrib.postgres.fields import ArrayField
from django.db import models
from fernet_fields import EncryptedCharField
from simple_history.models import HistoricalRecords

from back.apps.language_model.models.data import KnowledgeItem, KnowledgeBase


class AutoGeneratedTitle(models.Model):
    """
    An utterance is a synonym of an item.

    knowledge_item: KnowledgeItem
        The knowledge item this synonym refers to.
    title: str
        The synonym of the item.
    embedding: VectorField
        A computed embedding for the model.
    """

    knowledge_item = models.ForeignKey(KnowledgeItem, on_delete=models.CASCADE)
    title = models.TextField()
    embedding = ArrayField(models.FloatField(), blank=True, null=True)


class RAGConfig(models.Model):
    """
    It relates the different elements to create a RAG (Retrieval Augmented Generation) pipeline
    """
    name = models.CharField(max_length=255, unique=True)
    knowledge_base = models.ForeignKey(KnowledgeBase, on_delete=models.CASCADE)
    llm_config = models.ForeignKey("LLMConfig", on_delete=models.PROTECT)
    prompt_config = models.ForeignKey("PromptConfig", on_delete=models.PROTECT)
    generation_config = models.ForeignKey("GenerationConfig", on_delete=models.PROTECT)


class LLMConfig(models.Model):
    """
    A model config with all the settings to configure an LLM.
    name: str
        Just a name for the model.
    llm_type: str
        The type of LLM to use.
    llm_name: str
        The name of the LLM to use. It can be a HuggingFace repo id, an OpenAI model id, etc.
    ggml_model_filename: str
        The GGML filename of the model, if it is a GGML model.
    model_config: str
        The huggingface model config of the model, needed for GGML models.
    load_in_8bit: bool
        Whether to load the model in 8bit or not.
    trust_remote_code_tokenizer: bool
        Whether to trust the remote code for the tokenizer or not.
    trust_remote_code_model: bool
        Whether to trust the remote code for the model or not.
    revision: str
        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models
    """

    LLM_CHOICES = (
        ('local_cpu', 'CPU Model'), # GGML models optimized for CPU inference
        ('local_gpu', 'Local Model'), # Use locally (VLLM) or via HuggingFace for GPU inference
        ('vllm', 'VLLM Client'),   # Access VLLM engine remotely
        ('openai', 'OpenAI Model') # ChatGPT models from OpenAI
    )

    name = models.CharField(max_length=255, unique=True)
    llm_type = models.CharField(max_length=10, choices=LLM_CHOICES, default="local")
    llm_name = models.CharField(max_length=100, default="gpt2")
    ggml_llm_filename = models.CharField(max_length=255, blank=True, null=True)
    model_config = models.CharField(max_length=255, blank=True, null=True)
    load_in_8bit = models.BooleanField(default=False)
    use_fast_tokenizer = models.BooleanField(default=True)
    trust_remote_code_tokenizer = models.BooleanField(default=False)
    trust_remote_code_model = models.BooleanField(default=False)
    revision = models.CharField(max_length=255, blank=True, null=True, default="main")

    def __str__(self):
        return self.name


class PromptConfig(models.Model):
    """
    Defines the structure of the prompt for a model.
    system_prefix : str
        The prefix to indicate instructions for the LLM.
    system_tag : str
        The tag to indicate the start of the system prefix for the LLM.
    system_end : str
        The tag to indicate the end of the system prefix for the LLM.
    user_tag : str
        The tag to indicate the start of the user input.
    user_end : str
        The tag to indicate the end of the user input.
    assistant_tag : str
        The tag to indicate the start of the assistant output.
    assistant_end : str
        The tag to indicate the end of the assistant output.
        The tag to indicate the end of the role (system role, user role, assistant role).
    n_contexts_to_use : int, optional
        The number of contexts to use, by default 3
    lang : str, optional
        The language of the prompt, by default 'en'
    model : Model
        The model this prompt structure belongs to.
    """
    name = models.CharField(max_length=255, unique=True)
    system_prefix = models.TextField(blank=True, default="")
    system_tag = models.CharField(max_length=255, blank=True, default="")
    system_end = models.CharField(max_length=255, blank=True, default="")
    user_tag = models.CharField(max_length=255, blank=True, default="<|prompt|>")
    user_end = models.CharField(max_length=255, blank=True, default="")
    assistant_tag = models.CharField(max_length=255, blank=True, default="<|answer|>")
    assistant_end = models.CharField(max_length=255, blank=True, default="")
    n_contexts_to_use = models.IntegerField(default=3)
    history = HistoricalRecords()

    def __str__(self):
        return self.name


class GenerationConfig(models.Model):
    """
    Defines the generation configuration for a model.
    top_k : int, optional
        The number of tokens to consider for the top-k sampling, by default 50
    top_p : float, optional
        The cumulative probability for the top-p sampling, by default 1.0
    temperature : float, optional
        The temperature for the sampling, by default 1.0
    repetition_penalty : float, optional
        The repetition penalty for the sampling, by default 1.0
    seed : int, optional
        The seed for the sampling, by default 42
    max_new_tokens : int, optional
        The maximum number of new tokens to generate, by default 256
    model : Model
        The model this generation configuration belongs to.
    """
    name = models.CharField(max_length=255, unique=True)
    top_k = models.IntegerField(default=50)
    top_p = models.FloatField(default=1.0)
    temperature = models.FloatField(default=1.0)
    repetition_penalty = models.FloatField(default=1.0)
    seed = models.IntegerField(default=42)
    max_new_tokens = models.IntegerField(default=256)

    def __str__(self):
        return self.name
