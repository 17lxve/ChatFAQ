[{"model": "language_model.retrieverconfig", "pk": 1, "fields": {"created_date": "2023-10-06T11:58:11.051", "updated_date": "2023-10-06T11:58:11.053", "name": "default", "model_name": "intfloat/e5-small-v2", "retriever_type": "e5", "batch_size": 1, "device": "cpu"}},
{"model": "language_model.llmconfig", "pk": 1, "fields": {"created_date": "2023-10-06T11:58:11.036", "updated_date": "2023-10-06T11:58:11.038", "name": "default", "llm_type": "openai", "llm_name": "gpt-3.5-turbo", "ggml_llm_filename": null, "model_config": null, "load_in_8bit": false, "use_fast_tokenizer": true, "trust_remote_code_tokenizer": false, "trust_remote_code_model": false, "revision": "main", "model_max_length": null}},
{"model": "language_model.promptconfig", "pk": 1, "fields": {"created_date": "2023-10-06T11:58:11.040", "updated_date": "2023-10-06T11:58:11.042", "name": "default", "system_prefix": "- You are a helpful assistant chatbot.\r\n- You respond in the same language as the user question.\r\n- You answer questions with the context provided.\r\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user", "system_tag": "", "system_end": "", "user_tag": "<|prompt|>", "user_end": "", "assistant_tag": "<|answer|>", "assistant_end": "", "n_contexts_to_use": 5}},
{"model": "language_model.generationconfig", "pk": 1, "fields": {"created_date": "2023-10-06T11:58:11.016", "updated_date": "2023-10-06T11:58:11.019", "name": "default", "top_k": 50, "top_p": 1.0, "temperature": 0.2, "repetition_penalty": 1.0, "seed": 42, "max_new_tokens": 256}},
{"model": "language_model.knowledgebase", "pk": 1, "fields": {"created_date": "2024-04-19T13:19:09.842", "updated_date": "2024-04-19T13:19:09.842", "name": "test", "lang": "en"}},
{"model": "language_model.knowledgeitem", "pk": 1, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "Designed to scale in your complex business ecosystem  today", "content": "Designed to scale in your complex business ecosystem  today", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 2, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "Architected to stay in control", "content": "Architected to stay in control\nIt offers complete management over the service experience, from your customers input, up to the AI generative model and deployment architecture.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 3, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "Why is it for you?", "content": "Why is it for you?\nPlug-and-play for your business\nIt integrates seamlessly into your business organization by guaranteeing full compliance with your customer care, IT, security and legal constraints.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 4, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "Why it stands out", "content": "Why it stands out\nContinuously improved with latest AI innovations\nIt leverages on-going developments around generative Large Language Models LLM technology such as GPT, LLaMa, Falcon models to provide immediacy and response accuracy.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 5, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "What is it?", "content": "What is it?\nThe most advanced conversational interface\nIt is blending the traditional chat and search experiences into a unique service design that offers built-in chat, search and stand-alone experiences.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 6, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "ChatFAQ, a new AI assistant paradigm to boost your business", "content": "It, a new AI assistant paradigm to boost your business\n", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 7, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "Redefine your customer engagement with a generative-AI assistant", "content": "Redefine your customer engagement with a generative-AI assistant", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 8, "fields": {"created_date": "2024-04-19T13:19:10.766", "updated_date": "2024-04-19T13:19:10.766", "knowledge_base": 1, "title": "Architected to stay in control\n", "content": "Architected to stay in control\nIt offers complete management over the service experience, from your customers input, up to the AI generative model and deployment architecture.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 9, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Service management", "content": "Service management\nRead, review, measure all customers interactions. Incorporate their feedbacks to extend your knowledge database.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 10, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Tailored AI", "content": "Tailored AI\nFeed and train with your content to provide relevant information for your business and eliminate AI hallucinations.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 11, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Cloud provider agnostic", "content": "Cloud provider agnostic\nIt is prepared for standard cloud deployments so you can run service on the infrastructure of your choice.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 12, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Flexible SDK", "content": "Flexible SDK\nBrand and adapt the conversation experience to your business with personalised flows, tone and responses.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 13, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Zero license costs", "content": "Zero license costs\nNo license trap. It is an open-source solution, meaning your costs only grow as you scale your service infrastructure.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 14, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Service many languages", "content": "Service many languages\nProvide relevant answers in the preferred languages of your customers without compromising quality.", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 15, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "100% secure", "content": "100% secure\nIt has been designed to guarantee the maximum level of security and evolving worldwide regulation compliance\n- GDPR Ready\n- No third-party\n- No user data leakage\n- No black-box effect", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 16, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Transform any FAQ into a chat with\ngenerative-AI today!", "content": "Transform any FAQ into a chat with\ngenerative-AI today!\nGive your business the proper boost it deserves with It generative-AI assistant", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 17, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "ChatFAQ, An open-source conversational search platform as an alternative to commercial GPT solutions.\n", "content": "\"It, An open-source conversational search platform as an alternative to commercial GPT solutions.\n\"", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 18, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Funded by European Union – NextGenerationEU\n", "content": "It is Funded by European Union – NextGenerationEU\n\n", "url": "https://www.chatfaq.io/", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 19, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Transforming customer experience", "content": "Transforming customer experience\nExperience a new level of customer engagement with the It widget. Designed with flexibility and customer experience at its core, our widget blends seamlessly into your brand, offering a unique way to interact, assist, and engage.\n", "url": "https://www.chatfaq.io/features/widget", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 20, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Interactive design", "content": "Interactive design\nOur widget employs a user-friendly responsive design that makes it intuitive for customers to interact with, ensuring a smooth and positive experience.", "url": "https://www.chatfaq.io/features/widget", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 21, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Seemless integration", "content": "Seemless integration\nOur widget easily embeds into your web site or platform, offering immediate access to enhanced customer service.", "url": "https://www.chatfaq.io/features/widget", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 22, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Multilingual support", "content": "Multilingual support\nOur widget can communicate in multiple languages, thereby enabling businesses to serve a global customer base without compromising on service quality.", "url": "https://www.chatfaq.io/features/widget", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 23, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Fully brandable UX/UI", "content": "Fully brandable UX/UI\nCreate an experience that reflects your brand's uniqueness and caters to your audience's needs. The It widget offers full flexibility over all UI aspects (size, color, fonts, and logo) to align with your brand identity.", "url": "https://www.chatfaq.io/features/widget", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 24, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Trackable and usable conversations", "content": "Trackable and usable conversations\nWe understand the importance of accessibility and traceability. With the It widget, conversations are logged and accessible anytime, for both customers and administrators. This functionality enables consistent customer service and the ability to track and improve interactions based on past conversations.", "url": "https://www.chatfaq.io/features/widget", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 25, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Packed with advanced capabilities", "content": "Packed with advanced capabilities\nIt widget comes with many built-in features to give the best experience to your users: real-time response generation, negative and positive user voting flows, conversation history management. Soon, our widget will support standalone page functionality and a search bar module to streamline navigation and provide quicker access to information.", "url": "https://www.chatfaq.io/features/widget", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 26, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Communicate your brand with generative AI models", "content": "Communicate your brand with generative AI models\nIt leverages Generative Language models like Large Language Models (LLMs) to offer unprecedented capabilities in natural language processing and generation. This allows to deliver a sophisticated chatbot service that comprehends and generates human-like text and empowers brands to deliver personalized and efficient communication at scale.", "url": "https://www.chatfaq.io/features/generative-ai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 27, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Your Brand Business Domain", "content": "Your Brand Business Domain\nBy understanding the brand's business domain, the AI model can assist customers with more specific and tailored information, helping them make informed decisions and improving their perception of the brand's expertise and credibility.", "url": "https://www.chatfaq.io/features/generative-ai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 28, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Meeting your Customer Needs", "content": "Meeting your Customer Needs\nStay ahead of evolving customer demands to enhance the chatbot's functionality over time by identifying new business opportunities or areas for improvement based on customer feedback and interactions that chatFAQ will report to you.", "url": "https://www.chatfaq.io/features/generative-ai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 29, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Controlling Data privacy", "content": "Controlling Data privacy\nUnlike third-party SaaS chatbots, It implements robust data privacy measures, such as data encryption, self-hosting, and anonymization techniques to ensure user questions and company information are kept secure and confidential.", "url": "https://www.chatfaq.io/features/generative-ai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 30, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Automatic knowledge expansion", "content": "Automatic knowledge expansion\nUpload your business content as CSV or PDF files and It will generate utterances to expand your knowledge dataset and improve the AI model accuracy. If you don’t have any existing Frequently Asked Questions, no need to manually create them from scratch. It will infer FAQ automatically and prepare the training dataset covering your business context.", "url": "https://www.chatfaq.io/features/generative-ai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 31, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Powerful and secured natural language pipeline", "content": "Powerful and secured natural language pipeline\nIt pipeline is architected on a flexible multi-stage processing that transforms user questions into embedding vectors that are matched by an intent-correlation model. Once the user intent is understood and proper business context has been built, a custom prompt engineering is applied to perform NLG generation with hallucination and adversarial prompt preventions.", "url": "https://www.chatfaq.io/features/generative-ai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 32, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Best selection of AI models", "content": "Best selection of AI models\nRunning LLM models is a complex and costly task both on local or cloud deployments. Among the best open-source LLM models, It selects 2 models for development and production purpose in order to achieve the best performance (in number of generated tokens per second) with the minimum hardware footprint (memory and CPU/GPU requirements).", "url": "https://www.chatfaq.io/features/generative-ai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 33, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Experience a unified workflow with our built-in integrations", "content": "Experience a unified workflow with our built-in integrations\nIt’s powerful integration capabilities open the door to an elevated customer experience. By seamlessly interconnecting with your existing ecosystem, we enable enhanced functionalities across various touchpoints.", "url": "https://www.chatfaq.io/features/integrations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 34, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Ease of integration", "content": "Ease of integration\nCharFAQ plug-and-play system allows effortlessly integration into your existing framework, while complying with your business's customer care, IT, security and legal requirements.", "url": "https://www.chatfaq.io/features/integrations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 35, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Versatile compatibility", "content": "Versatile compatibility\nIt's integration ability ensures that all your front-end and back-end systems work in harmony, creating a seamless customer experience.", "url": "https://www.chatfaq.io/features/integrations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 36, "fields": {"created_date": "2024-04-19T13:19:10.767", "updated_date": "2024-04-19T13:19:10.767", "knowledge_base": 1, "title": "Scalability", "content": "Scalability\nOur chat interface is designed to scale, fitting perfectly with your growing business needs and easily adapting to changing customer expectations.", "url": "https://www.chatfaq.io/features/integrations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 37, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "ChatFAQ SDK: a foundation for integration", "content": "It SDK: a foundation for integration\nWith our flexible SDK, you have the tools at your disposal to create a personalized interaction experience. Tailor the conversation flow, adapt the tone, and set up custom responses that align with your brand. Acting as the cornerstone of our integration abilities, the SDK is adaptable across your preferred platforms, ensuring a consistent brand voice and customer experience.", "url": "https://www.chatfaq.io/features/integrations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 38, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "Versatile compatibility", "content": "Versatile compatibility\nOur service blends into your existing tools, ensuring seamless integration with your CRM, ERP, CLOUD, and CMS systems. This minimizes disruptions, fosters efficiency, and supports even the most complex architectures. It works harmoniously with a broad range of platforms such as Salesforce, HubSpot, Microsoft Dynamics, ServiceNow, Magento, Shopify, Hybris Commerce , and more, ensuring a streamlined operation across your business ecosystem.", "url": "https://www.chatfaq.io/features/integrations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 39, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "Elevate your user interface", "content": "Elevate your user interface\nEnrich your front-end user interface with the integration of It. Our front-end integration capabilities allow for the creation of an engaging and personalized customer experience across various platforms. Apart from our web widgets, you can easily plug internal communication tools like Slack and Teams, or external platforms such as WhatsApp and Messenger, It ensures you're connected wherever your customers are.", "url": "https://www.chatfaq.io/features/integrations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 40, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "Contact us", "content": "[Contact Us](https://www.chatfaq.io/contact-us)\nGet In Touch With Us\nPlease provide the requested information below and leave your message in the designated field. We will get back to you as soon as possible.\nThank you for reaching out to us!", "url": "https://www.chatfaq.io/contact-us", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 41, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "ChatFAQ news", "content": "[It news](https://www.chatfaq.io/blog)\nCheck our last events, updates and articles about It ", "url": "https://www.chatfaq.io/blog", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 42, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "init > chatfaq-s-documentation", "content": "init > chatfaq-s-documentation: Welcome to the official documentation of It, a comprehensive open-source community-driven platform for creating AI chatbots.\n\nIf you are new to this documentation, we recommend that you read the [introduction](https://chatfaq.readthedocs.io/en/latest/introduction.html) page to get an overview of what this documentation has to offer.\n\nThe table of contents in the sidebar should let you easily access the documentation for your topic of interest. You can also use the search function in the top-left corner.", "url": "https://chatfaq.readthedocs.io/en/latest/index.html#chatfaq-s-documentation", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 43, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "init > get-involved", "content": "init > get-involved: It is an open source project developed by a community of volunteers. The documentation team can always use your feedback and help to improve the tutorials and class reference. If you don’t understand something, or cannot find what you are looking for in the docs, help us make the documentation better by letting us know!\n\nSubmit an issue or pull request on the [GitHub repository](https://github.com/It/It/)", "url": "https://chatfaq.readthedocs.io/en/latest/index.html#get-involved", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 44, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "introduction > installation", "content": "introduction > installation: The system comprises three main components that you need to install:\n\nIt Components: Widget, Backend and SDK\n\nThe back-end [install](https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#back-installation) manages the communication between all the components. It also houses the database for storing all the data related to the chatbots, datasets, models, etc…\n\nThe SDK [install](https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#sdk-installation) launches a Remote Procedure Call (RPC) server to execute transitions and events from the posted FSM definitions.\n\nThe widget [install](https://chatfaq.readthedocs.io/en/latest/introduction.html#:~:text=The%20system%20comprises,with%20the%20bot.) is a JS browser client application from which the user interacts with the bot.", "url": "https://chatfaq.readthedocs.io/en/latest/introduction.html#installation", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 45, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "introduction > model-configuration", "content": "introduction > model-configuration: After setting up the components, you will probably want to configure a model that you want to use for your chatbot. Typically the model will be used from the SDK, from a state within its FSM.\n\nHere is an example of a minimum model [configuration](https://chatfaq.readthedocs.io/en/latest/introduction.html#:~:text=After%20setting%20up,model%20(configuration))", "url": "https://chatfaq.readthedocs.io/en/latest/introduction.html#model-configuration", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 46, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "introduction > quick-start", "content": "introduction > quick-start: Learning [how to use the SDK](https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html#usage) is the only requirement to start building your own chatbots with It.", "url": "https://chatfaq.readthedocs.io/en/latest/introduction.html#quick-start", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 47, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > back-installation", "content": "modules > installations > back-installation: This is It’s core component, the orchestrator of ChatGPT. It manages all the widgets and SDks connections, session storage, datasets and models registration, FSM registration, FSM executions (intended only for simple FSMs), etc…\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#back-installation", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 48, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > back-installation > setting-it-up-locally > prerequisites", "content": "modules > installations > back-installation > setting-it-up-locally > prerequisites: Make sure the next list of packages are installed on your system:\n\n- Python 3.10\n\n- python3.10-dev\n\n- python3.10-distutils\n\n- PostgreSQL\n\n- pgvector\n\n- gdal-bin\n\n- poetry", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#prerequisites", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 49, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > back-installation > installation > local-build > set-up", "content": "modules > installations > back-installation > installation > local-build > set-up: Install project dependencies:\n\n```\npoetry install\n```\n\nCreate a “chatfaq” database in PostgreSQL\n\n```\nsudo -u postgres psql -c \"CREATE DATABASE chatfaq\"\n```\n\nCreate a “chatfaq” user in PostgreSQL\n```\nsudo -u postgres psql -c \"CREATE user chatfaq WITH encrypted password 'chatfaq';\"\n```\n\nGive the newly created user the necessary privileges\n\n```\nsudo -u postgres psql -c \"grant all privileges on database chatfaq to chatfaq;\"\n```\n\nApply django migrations\n\n```\npoetry run ./manage.py migrate\n```\n\nApply fixtures\n\n```\nmake apply_fixtures\n```\n\nCreate a superuser\n\n```\n./manage.py createsuperuser --rpc_group 1\n```\n\nWhen creating the superuser notice that we are passing the `--rpc_group 1` flag. This is critical to be able to create an RPC Server with this same user later on.\n\nNow you can access the Django admin panel at http://localhost:8000/back/admin/ and login with the superuser credentials, from there you can CRUD all the relevant models.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#set-up", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 50, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > back-installation > installation > local-build > run", "content": "modules > installations > back-installation > installation > local-build > run: First of all, create a `.env` file with the needed variables set. You can see an example of those on [.env_example](.env_example) file. Next you can see the explanation of each variable:\n\n`DEBUG`: Set to \"yes\" to enable debug mode\n\n`SECRET_KEY`: Server secret key. This is used to provide cryptographic signing, and should be set to a unique, unpredictable value.\n\n`DATABASE_URL`: Database connection URL. This is the URL that will be used to connect to the database. It should be in the following format: `postgres://USER:PASSWORD@HOST:PORT/NAME`\n\n`BASE_URL`: Base URL of the server. This is the URL that will be used to connect to the server. It should be in the following format: `http://HOST:PORT`\n\n`AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` + `AWS_STORAGE_BUCKET_NAME` + `DO_REGION` + `STORAGES_MODE` + `STORAGE_MAKE_FILES_PUBLIC`: These variables are used to configure the storage backend. If you want to use AWS S3, you should set `STORAGES_MODE` to \"s3\" and set the other variables accordingly. If you want to use Digital Ocean Spaces, you should set `STORAGES_MODE` to \"spaces\" and set the other variables accordingly. If you want to use the local filesystem, you should set `STORAGES_MODE` to \"local\".\n\n`TG_TOKEN`, `WHATSAPP_TOKEN`, `SIGNAL_TOKEN`, `FB_TOKEN`: These variables are used to configure the messaging platforms. You should set the token of the platforms you want to use. If you don't want to use a platform (ie: you are using our [Widget](../widget/README.md) solution), you can leave its token empty.\n\nRun the server\n\n    make run", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#run", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 51, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > back-installation > setting-it-up-locally > docker", "content": "modules > installations > back-installation > setting-it-up-locally > docker: Alternatively you can simply run the server using docker.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#docker", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 52, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > installation > docker > build", "content": "modules > installations > installation > docker > build:     docker build -t chatfaq-back .", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#build", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 53, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > installation > docker > build > run", "content": "modules > installations > installation > docker > build > run:     docker run -p 8000:8000 chatfaq-back", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id1", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 54, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > usage > useful-endpoints", "content": "modules > installations > usage > useful-endpoints: Admin: [http://localhost:8000/back/admin/](http://localhost:8000/back/admin/)\n\nAuth Token Generation: [http://localhost:8000/back/api/login/](http://localhost:8000/back/api/login/)\n\nSwagger Docs: [http://localhost:8000/back/api/schema/swagger-ui/](http://localhost:8000/back/api/schema/swagger-ui/)\n\nRedoc Docs: [http://localhost:8000/back/api/schema/redoc/](http://localhost:8000/back/api/schema/redoc/)", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#useful-endpoints", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 55, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation", "content": "modules > installations > sdk-installation: For those chatbots with complex Finite State Machine (FSM) behaviours, you will probably want to run them on a separate process, that is what for the SDK is made for. Its primary function is to execute the FSM's computations (transition's conditions and states) by running Remote Procedure Call (RPC) server that listen to the back-end requests.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#sdk-installation", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 56, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation > prerequisites", "content": "modules > installations > sdk-installation > prerequisites: Make sure the next list of packages are installed on your system:\n\n- Python 3.10\n- python3.10-dev\n- python3.10-distutils\n- poetry", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id2", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 57, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation > pypi", "content": "modules > installations > sdk-installation > pypi:     poetry add chatfaq-sdk", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#pypi", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 58, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation > local-build > set-up", "content": "modules > installations > sdk-installation > local-build > set-up: Install project dependencies:\n\n    poetry install", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id5", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 59, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation > local-build > run", "content": "modules > installations > sdk-installation > local-build > run: First of all, create a `.env` file with the needed variables set. You can see an example of those on [.env_example](.env_example) file. Next you can see the explanation of each variable:\n\n`CHATFAQ_RETRIEVAL_HTTP`: The address for the HTTP of the back-end server.\n\n`CHATFAQ_BACKEND_WS`: The address for the WS of the back-end server.\n\n`CHATFAQ_TOKEN`: The token to authenticate with the back-end server. You can retrieve the auth token from the backend server:\n\n`curl -X POST -u username:password http://localhost:8000/back/api/login/`\n\nRun the example:\n\n    make run_example\n\nThis will run the example FSM that is located in [./examples/model_example/__init__.py](./examples/model_example/__init__.py) file. You can modify this file to test your own FSMs.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id6", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 60, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation > docker", "content": "modules > installations > sdk-installation > docker: Alternatively you can simply run the server using docker.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id7", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 61, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation > docker > build", "content": "modules > installations > sdk-installation > docker > build:     docker build -t chatfaq-sdk .", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id8", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 62, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > sdk-installation > docker > run", "content": "modules > installations > sdk-installation > docker > run:     docker run chatfaq-sdk", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id9", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 63, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > widget-installation", "content": "modules > installations > widget-installation: We built for you a custom front-end solution just so you can talk with your chatbot from the browser using an app you own. Although you can also connect any other message platform as such WhatsApp, Telegram, Signal, Facebook messenger, etc... It supports them all and if it doesn't it can easily be extended to do so.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#widget-installation", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 64, "fields": {"created_date": "2024-04-19T13:19:10.768", "updated_date": "2024-04-19T13:19:10.768", "knowledge_base": 1, "title": "modules > installations > widget-installation > prerequisites", "content": "modules > installations > widget-installation > prerequisites: Make sure the next list of packages are installed on your system:\n\n- npm\n- node v19.6.0", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id10", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 65, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > installations > widget-installation > instalaltion > npm", "content": "modules > installations > widget-installation > instalaltion > npm:     npm install chatfaq-widget", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#npm", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 66, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > installations > widget-installation > instalaltion > unpkg", "content": "modules > installations > widget-installation > instalaltion > unpkg:     <script src=\"unpkg.com/chatfaq-widget/dist/widget-loader.min.esm\"></script>", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#unpkg", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 67, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > installations > widget-installation > instalation > local-build > set up", "content": "modules > installations > widget-installation > instalation > local-build > set up: Install project dependencies:\n\n    npm i", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id13", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 68, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > installations > widget-installation > run", "content": "modules > installations > widget-installation > run: First of all, create a `.env` file with the needed variables set. You can see an example of those on [.env_example](.env_example) file. Next you can see the explanation of each variable:\n\n`CHATFAQ_BACKEND_API`: The address for the HTTP of the back-end server.\n\n`CHATFAQ_BACKEND_WS`:  The address for the WS of the back-end server.\n\nRun the example:\n\n    npm run dev\n\nThis will run a node server which will serve an empty webpage with just the Widget integrated on it, if you navigate to http://localhost:3000", "url": "https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id14", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 69, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > sdk > sdk-documentation", "content": "modules > sdk > sdk-documentation: For those chatbots with complex Finite State Machine (FSM) behaviours, you will probably want to run them on a separate process, that is what for the SDK is made for. Its primary function is to execute the FSM's computations (transition's conditions and states) by running Remote Procedure Call (RPC) server that listen to the back-end requests.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 70, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > sdk > sdk-documentation > usage > simple-example", "content": "modules > sdk > sdk-documentation > usage > simple-example: This is just a dummy example that displays the basic usage of the library.\n\nWe are going to build the next FSM:\n\n![fsm](../../../../doc/source/_static/images/simple_fsm_diagram.png)\n\nImport basic modules to build your first FMS:\n\n```python\nimport os\nimport random\nfrom chatfaq_sdk import ItSDK\nfrom chatfaq_sdk.fsm import FSMDefinition, State, Transition\nfrom chatfaq_sdk.conditions import Condition\nfrom chatfaq_sdk.layers import Text\n```\n\nDeclare the 3 possible states of our FSM:\n\n\n```python\ndef send_greeting(ctx: dict):\n    yield Text(\"Hello!\")\n    yield Text(\"How are you?\", allow_feedback=False)\n\ngreeting_state = State(name=\"Greeting\", events=[send_greeting], initial=True)\n\n\ndef send_answer(ctx: dict):\n    last_payload = ctx[\"last_mml\"][\"stack\"][0][\"payload\"]\n    yield Text(\n        f'My answer to your message: \"{last_payload}\" is: {random.randint(0, 999)}'\n    )\n    yield Text(f\"Tell me more\")\n\nanswering_state = State(\n    name=\"Answering\",\n    events=[send_answer],\n)\n\n\ndef send_goodbye(ctx: dict):\n    yield Text(\"Byeeeeeeee!\", allow_feedback=False)\n\ngoodbye_state = State(\n    name=\"Goodbye\",\n    events=[send_goodbye],\n)\n\n```\n\nDeclare the only computable condition for the transitions of our FSM:\n\n\n```python\ndef is_saying_goodbye(ctx: dict):\n    if ctx[\"last_mml\"][\"stack\"][0][\"payload\"] == \"goodbye\":\n        return Condition(1)\n    return Condition(0)\n```\n\nNow lets glue everything together:\n\nDeclare our transitions\n\n```python\nany_to_goodbye = Transition(dest=goodbye_state, conditions=[is_saying_goodbye])\n\ngreeting_to_answer = Transition(\n    source=greeting_state,\n    dest=answering_state,\n    unless=[is_saying_goodbye],\n)\nanswer_to_answer = Transition(\n    source=answering_state, dest=answering_state, unless=[is_saying_goodbye]\n)\n```\n\nBuild the final instance of our FSM:\n\n```python\nfsm_definition = FSMDefinition(\n    states=[greeting_state, answering_state, goodbye_state],\n    transitions=[greeting_to_answer, any_to_goodbye, answer_to_answer],\n)\n```\n\nFinally, run the RPC server loop with the previously built FSM:\n\n```python\nimport os\n\nsdk = ItSDK(\n    chatfaq_retrieval_http=\"http://localhost:8000\",\n    chatfaq_ws=\"ws://localhost:8000\",\n    token=os.getenv(\"CHATFAQ_TOKEN\"),\n    fsm_name=\"my_first_fsm\",\n    fsm_definition=fsm_definition,\n)\nsdk.connect()\n```\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html#simple-example", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 71, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > sdk > sdk-documentation > usage > model-example", "content": "modules > sdk > sdk-documentation > usage > model-example: All of that is great, but where is the large language model capabilities that It offers?\n\nWhat if we want to build a FSM that makes use of a Language Model?\n\nFor that, you first need to [configure your model](../configuration/index.md).\n\nOnce you have configured all the components of the model, you will just need to reference the name of your RAG Configuration inside a state of the FSM.\n\nFor example, if you have a RAG Configuration named `my_rag_config`, you can use it inside a state like this:\n\n```python\nfrom chatfaq_sdk.fsm import FSMDefinition, State, Transition\nfrom chatfaq_sdk.layers import LMGeneratedText, Text\n\n\ndef send_greeting(ctx: dict):\n    yield Text(\"How can we help you?\", allow_feedback=False)\n\n\n\ndef send_answer(ctx: dict):\n    last_payload = ctx[\"last_mml\"][\"stack\"][0][\"payload\"]\n    yield LMGeneratedText(last_payload, \"my_rag_config\")\n\ngreeting_state = State(name=\"Greeting\", events=[send_greeting], initial=True)\n\nanswering_state = State(\n    name=\"Answering\",\n    events=[send_answer],\n)\n\n_to_answer = Transition(\n    dest=answering_state,\n)\n\nfsm_definition = FSMDefinition(\n    states=[greeting_state, answering_state],\n    transitions=[_to_answer]\n)\n```\n\nFor the sake of completeness, here is the diagram of this FSM:\n\n![fsm](../../../../doc/source/_static/images/model_fsm_diagram.png)\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html#model-example", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 72, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > widget", "content": "modules > widget: We built for you a custom front-end solution just so you can talk with your chatbot from the browser using an app you own. Although you can also connect any other message platform as such WhatsApp, Telegram, Signal, Facebook messenger, etc... It supports them all and if it doesn't it can easily be extended to do so.\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 73, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > widget > usage > js-library", "content": "modules > widget > usage > js-library: \n```html\n<div id=\"chatfaq-widget\"></div>\n\n<script>\n    import { ChatfaqWidget } from \"chatfaq-widget/dist/widget-loader.esm\";\n\n    const config = {\n        element: \"#chatfaq-widget\",\n        chatfaqApi: \"http://127.0.0.1:8000\",\n        chatfaqWs: \"ws://127.0.0.1:8000\",\n        userId: 1234567890,\n        fsmDef: \"simple_fsm\",\n        title: \"Hello there 👋\",\n        subtitle: \"How can we help you?\",\n        historyOpened: true,\n        maximized: false\n    }\n\n    const chatfaqWidget = new ChatfaqWidget(config);\n\n</script>\n```\n\nIt is also possible to pass the config keys as data attributes to the mounted element as such:\n\n```html\n<div\n    id=\"chatfaq-widget\"\n    data-chatfaq-api=\"http://127.0.0.1:8000\"\n    data-chatfaq-ws=\"ws://127.0.0.1:8000\"\n    user-id=\"1234567890\"\n    data-fsm-def=\"simple_fsm\"\n    data-title=\"Hello there 👋\"\n    data-subtitle=\"How can we help you?\"\n    history-opened=\"true\"\n    maximized=\"false\"\n></div>\n```\nIf you declare data attributes and a config object and its keys collide, then the config object will have priority.\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#js-library", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 74, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > widget > usage > web-component", "content": "modules > widget > usage > web-component: ```html\n\n<script>\n    import { ChatfaqWidgetCustomElement } from \"chatfaq-widget/dist/widget-loader.esm\";\n    customElements.define(\"chatfaq-widget\", ChatfaqWidgetCustomElement)\n</script>\n\n<chatfaq-widget\n    data-chatfaq-api=\"http://127.0.0.1:8000\"\n    data-chatfaq-ws=\"ws://127.0.0.1:8000\"\n    data-user-id=\"1234567890\"\n    data-fsm-def=\"simple_fsm\"\n    data-title=\"Hello there 👋\"\n    data-subtitle=\"How can we help you?\"\n    data-history-opened=\"true\"\n    data-maximized=\"false\"\n></chatfaq-widget>\n```", "url": "https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#web-component", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 75, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > widget > usage > widget-params", "content": "modules > widget > usage > widget-params: Next we will explain all the widget's possible parameters:\n\n`element`: string selector or HTMLElement to which the widget will be attached.\n\n`chatfaqApi`: url of the chatfaq-api.\n\n`chatfaqWs`: url of the chatfaq-ws.\n\n`userId`: In case you want to keep track of the user's conversations, you can pass a userId to the widget. This id will be store as a cookie and will be sent to the backend on each request. Later on the widget will be able to retrieve the conversations history of the user.\n\n`fsmDef`: name of the FSM definition to use.\n\n`title`: title which will appear on the header of the chatbot\n\n`subtitle`: subtitle which will appear on the footer of the chatbot\n\n`historyOpened`: if the widget starts with the left menu opened.\n\n`maximized`: if the widget starts maximized.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#widget-params", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 76, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > widget > usage > widget-styles", "content": "modules > widget > usage > widget-styles: We made the widget styles hightly customizable by exposing a set of variables that controls greatly the look and feel if it. You can easely overwrite them as shown in the next example:\n\n```html\n\n<script type=\"text/javascript\" src=\"chatfaq-widget/dist/widget-loader.esm\"></script>\n<style>\n    :root {\n        --chatfaq-color-primary-200: red;\n        --chatfaq-color-secondary-pink-500: blue;\n        --chatfaq-color-tertiary-blue-500: green;\n        --chatfaq-color-tertiary-green-500: black;\n    }\n</style>\n```\n\nYou can find the full list of variables on [widget/assets/styles/_variables.css](../../../../widget/assets/styles/_variables.css)", "url": "https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#widget-styles", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 77, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > init", "content": "modules > ia-configuration > init: After setting up the components, you will probably want to configure a model that you want to use for your chatbot. Typically the model will be used from the SDK, from a state within its FSM.\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 78, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline", "content": "modules > ia-configuration > the-rag-pipeline: The [RAG (Retrieval-Augmented Generation)](https://arxiv.org/abs/2005.11401) architecture is a workflow that combines a retriever and a generator. The retriever is used to retrieve the most relevant knowledge items from a knowledge base, and the generator is used to generate an answer from the retrieved knowledge items.\n\nIn It we choose to follow this pattern. Next we explain how to configure it.\n\nWe define 5 main components that are needed to configure a RAG pipeline:\n\n- Knowledge Base\n- Retriever\n- Prompt\n- Generation\n- LLM\n- RAG\n\nIt provide in its fixtures a default configuration for each of these components except for the Knowledge Base and the RAG Config. You can apply the fixtures by simply running the following command from the `back` directory:\n\n```bash\nmake apply_fixtures\n```\n\nWe also provide an example of each component here, so you can use it as a reference.\n\nCurrently all the relevant data/models can be accessed and modified from the Django admin panel ([http://localhost/back/admin/](http://localhost/back/admin/)) or from the CLI.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#the-rag-pipeline", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 79, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > knowledge-base", "content": "modules > ia-configuration > the-rag-pipeline > knowledge-base: The knowledge base is your source of truth.\n\nIt typically starts with a collection of documents (CSVs, PDFs or URLs) that the chatbot will use to answer questions.\n\nOnce a knowledge base is created, the system will parse your source document and generate the corresponding knowledge items.\n\nNext we list the different properties that a of knowledge bases has.\n\n- **lang**: The language of the knowledge base. It is used to tokenize the documents.\n- **original_csv**: The CSV file.\n- **original_pdf**: The PDF file.\n- **original_url**: The URL.\n\n<b> CSV parsing options</b>\n\n- **csv_header**: Whether the CSV file has a header or not.\n- **title_index_col**: The index of the column that contains the title of the knowledge item.\n- **content_index_col**: The index of the column that contains the content of the knowledge item.\n- **url_index_col**: The index of the column that contains the URL of the knowledge item.\n- **section_index_col**: The index of the column that contains the section of the knowledge item.\n- **role_index_col**: The index of the column that contains the role of the knowledge item.\n- **page_number_index_col**: The index of the column that contains the page number of the knowledge item.\n\n<b> PDF and URL parsing options</b>\n\n- **strategy**: The strategy to use to parse the PDF files. Can be 'auto', 'fast', 'ocr' or 'high_res'. Default: 'fast'.\n- **recursive**: Whether to recursively parse the URLs or not. Default: True.\n- **splitter**: The splitter used to split the documents into chunks. It is used to generate the knowledge items. Can be 'sentences', 'words', 'tokens' and 'smart'. Default: 'sentences'.\n- **chunk_size**: The number of tokens per chunk. It is used by the splitter to split the documents into chunks. Default: 128.\n- **chunk_overlap**: The number of tokens that overlap between two chunks. Default: 16.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#knowledge-base", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 80, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > knowledge-base > recommendations", "content": "modules > ia-configuration > the-rag-pipeline > knowledge-base > recommendations: - The **strategy** to use depends on the time that you want to wait for the parsing process to finish and the quality of the parsing process. The strategies are ordered from fastest to slowest and from worst quality to best quality. The 'fast' strategy is the default one and it is the one that we recommend for most use cases, it only lasts a few seconds and it has a good quality. The 'high_res' strategy is the one with the best quality but it can last several minutes. For more information about the different strategies check [here](https://unstructured-io.github.io/unstructured/bricks/partition.html#partition-pdf).\n\n- The **splitter** that we recommend is the 'sentences' one. It splits the documents into sentences and then it merges the sentences into chunks of approximately 'chunk_size' tokens. This is the best option for most use cases. The 'smart' splitter uses GPT-4 to split the documents into semantic meaningful chunks, this is **VERY EXPENSIVE** and can bump into OpenAI rate limits (will be optimized in the future).\n\n- The **chunk_size** that we recommend is 128. This is the default value and it is the one that we recommend for most use cases, it is a good balance between retrieval quality and information density. If you want to increase the retrieval quality you can decrease this value, but it will decrease the information density of a chunk.\n\n- The **chunk_overlap** is used when splitting with the 'words' or 'tokens' splitters. 16 or 32 is enough for not losing information between chunks.\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#recommendations", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 81, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > knowledge-base > csv structure", "content": "modules > ia-configuration > the-rag-pipeline > knowledge-base > csv structure: An example of a CSV for the Knowledge Base is the following:\n\n| title | content | url | section | role |\n| --- | --- | --- | --- | --- |\n| Can It integrate with communication tools like Slack and Teams? | Yes, It can integrate with communication tools like Slack and Teams, enhancing your communication capabilities and enabling seamless interactions with your audience. | https://www.chatfaq.io/features/integrations | Features > Integrations | user |\n| Can the It Widget be tailored to fit specific brand identities? | Absolutely, the It Widget can be fully branded to reflect your brand's uniqueness, including size, color, fonts, and logo. This ensures it aligns perfectly with your brand identity. | https://www.chatfaq.io/features/widget | Features > Widget | user |\n| Does It offer a customized Natural Language Processing (NLP) engine? | Yes, It includes a specialized NLP/NLG engine that enhances the conversational capabilities of chatbots, making them more effective in understanding and responding to user queries. | https://github.com/It/It | GitHub > Documentation | user |\n| Does It offer specific enterprise solutions? | Indeed, It is suitable for businesses and can be tailored to meet enterprise needs. It offers features and customization options suitable for businesses of all sizes. | https://github.com/It/It | GitHub > About | user |\n| Does the It Widget support multiple languages? | Yes, the It Widget is multilingual, allowing businesses to communicate with a global customer base while maintaining service quality. | https://www.chatfaq.io/features/widget | Features > Widget | user |\n| How can I customize the user interface of the It Widget? | The It Widget offers complete flexibility over UI aspects, including size, color, fonts, and logo, to align with your brand's uniqueness and cater to your audience's needs. | https://www.chatfaq.io/features/widget | Features > Widget | user |\n| How can I expand my knowledge dataset with It? | You can expand your knowledge dataset with It by uploading your business content as CSV or PDF files. It will automatically generate utterances to enhance your knowledge dataset, improving the accuracy of the AI model. Even if you don't have existing Frequently Asked Questions, It can infer FAQs and prepare a training dataset covering your business context. | https://www.chatfaq.io/features/generative-ai | Features > Generative AI | user |\n| ... | ... | ... | ... | ... |", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#csv-structure", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 82, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > retriever-config", "content": "modules > ia-configuration > the-rag-pipeline > retriever-config: The retriever is the component that will retrieve the most relevant knowledge items from the knowledge base.\n\nThe retriever is configured with the following properties:\n\n- **name**: Just a name for the retriever.\n- **model_name**: The name of the retriever model to use. It must be a HuggingFace repo id. Default: 'intfloat/e5-small-v2'.\n- **batch_size**: The batch size to use for the retriever. Default: 1.\n- **device**: The device to use for the retriever. It can be a CPU or a GPU. Default: 'cpu'.\n\nWe recommend setting the **model_name** to one of the [e5 family models](https://huggingface.co/intfloat). The retriever is developed with these models as the base, so it will work better with them. We suggest to use [intfloat/e5-small-v2](https://huggingface.co/intfloat/e5-small-v2) for English and [intfloat/multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) for other languages.\n\nFor **batch_size** we recommend using 1 for CPU and for GPU as much as your GPU can handle. For personal use, batch size of 1 is enough, but for production use, you should use a higher batch size and a GPU.\n\nFor **device** we recommend using a GPU if you have one available. For personal use it is enough to use a CPU, but for production use, you should use a GPU.\n\nAn example of a retriever config is the following:\n\n```json\n{\n    \"name\": \"e5-retriever\",\n    \"model_name\": \"intfloat/e5-small-v2\",\n    \"batch_size\": 1,\n    \"device\": \"cpu\"\n}\n```", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#retriever-config", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 83, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > llm-config", "content": "modules > ia-configuration > the-rag-pipeline > llm-config: The LLM is the component that defines the model that will generate the answer from the prompt.\n\nThe LLM is configured with the following properties:\n\n- **name**: Just a name for the LLM.\n- **llm_type**: The type of LLM to use. It can be 'OpenAI', 'Local GPU Model' (HuggingFace), 'Local CPU Model (ggml)' or a 'vLLM Client'. Default: 'Local GPU Model'.\n- **llm_name**: The name of the LLM to use. It can be a HuggingFace repo id, an OpenAI model id, etc. Default: gpt2.\n- **ggml_llm_filename**: The GGML filename of the model, if it is a GGML model.\n- **model_config**: The huggingface model config of the model, needed for GGML models.\n- **load_in_8bit**: Whether to load the model in 8bit or not, only for Local GPU HuggingFace models. Default: False.\n- **use_fast_tokenizer**: Whether to use the fast tokenizer or not. Default: True.\n- **trust_remote_code_tokenizer**: Whether to trust the remote code for the tokenizer or not. Default: False.\n- **trust_remote_code_model**: Whether to trust the remote code for the model or not. Default: False.\n- **revision**: The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models. Default: main.\n- **model_max_length**: The maximum length of the model. Default: None.\n\n\nOur preferred option is to use an open-source LLM like [Llama-2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) for English and [Qwen-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat) for other languages. \n\nTo access Llama-2 models you need to set a environment variable in the back `.env` file with your HuggingFace API key:\n```\nHUGGINGFACE_KEY=XXXXXX\n```\n\n\nWe can run these models locally, using a GPU or a CPU. ", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#llm-config", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 84, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > llm-config > gpu", "content": "modules > ia-configuration > the-rag-pipeline > llm-config > gpu: For GPU we recommend using the following configuration:\n```json\n{\n    \"name\": \"Llama2_GPU\",\n    \"llm_type\": \"Local GPU Model\",\n    \"llm_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"revision\": \"main\",\n    \"load_in_8bit\": false,\n    \"use_fast_tokenizer\": true,\n    \"trust_remote_code_tokenizer\": false,\n    \"trust_remote_code_model\": false,\n    \"model_max_length\": 4096\n}\n```\nThis uses the HuggingFace model implementations.\n\n> ⚠️ To know if our GPU is enough to load the model we need to multiply its number of parameters by 2. For example, Llama-2-7B has 7B parameters, so we need at least 14GB of GPU memory to load it. This is because every parameter is stored in 2 bytes.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#gpu", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 85, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > llm-config > cpu", "content": "modules > ia-configuration > the-rag-pipeline > llm-config > cpu: For CPU we recommend using the following configuration:\n```json\n{\n    \"name\": \"Llama2_CPU\",\n    \"llm_type\": \"Local CPU Model (ggml)\",\n    \"llm_name\": \"TheBloke/Llama-2-7B-GGML\",\n    \"revision\": \"main\",\n    \"ggml_llm_filename\": \"llama-2-7b.ggmlv3.q4_0.bin\",\n    \"model_config\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"use_fast_tokenizer\": true,\n    \"trust_remote_code_tokenizer\": false,\n    \"trust_remote_code_model\": false,\n    \"model_max_length\": 4096\n}\n```\n\nThis uses the [GGML](https://github.com/ggerganov/ggml/) library and [CTransformers](https://github.com/marella/ctransformers/tree/main) for python bindings. For a list of available models, check [here](https://github.com/marella/ctransformers/tree/main#supported-models).\n\nFor these configurations we need to specify the repo where the models files are stored (llm_name) and then the filename of the model file (ggml_llm_filename). We also need to specify the model config, which is the HuggingFace model config of the model.\n\n> ⚠️ To know if our CPU is enough to run the model we need to divide its number of parameters by 2. For example, Llama-2-7B has 7B parameters, so we need at least 3.5GB of RAM to run it. This is because it uses 4 bit quantization and every parameter is stored in 4 bits.\n>\n> Given that our prompts are long, the time to get the first word can be long (several seconds), but after that the generation is fast.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#cpu", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 86, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > llm-config > openai", "content": "modules > ia-configuration > the-rag-pipeline > llm-config > openai: This is the easiest way to get a model running. We just need to specify the model type and the model name. For example:\n```json\n{\n    \"name\": \"ChatGPT\",\n    \"llm_type\": \"OpenAI\",\n    \"llm_name\": \"gpt-3.5-turbo\"\n}\n```\n\nThe OpenAI models are specified [here](https://platform.openai.com/docs/models/). `gpt-3.5-turbo` should be enough for most use cases. To access OpenAI models you need to set a environment variable in the back `.env` file with your OpenAI API key:\n```\nOPENAI_API_KEY=XXXXXX\n```", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#openai", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 87, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > llm-config > vllm-client", "content": "modules > ia-configuration > the-rag-pipeline > llm-config > vllm-client: This uses a client to connect to a [vLLM server](https://github.com/vllm-project/vllm). The vLLM server is a server that runs a LLM model and exposes an API to generate answers, it has the best latency and throughput performance. \n\nTo configure this server you need to:\n\n- You need to specify the model that you want to use inside this [`Dockerfile`](https://github.com/It/It/tree/develop/model_engines/llm), and then follow [this instructions](https://github.com/It/It/tree/develop/model_engines/llm).\n- - You need to specify the URL of the vLLM server in the `.env`. Usually it will be `VLLM_ENDPOINT_URL=http://localhost:5000/generate`.\n- Start the back, go to the admin and from it you only need to specify that you want to use the `vLLM Client`.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#vllm-client", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 88, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > prompt-config", "content": "modules > ia-configuration > the-rag-pipeline > prompt-config: The prompt is the input that the LLM will use to generate the answer. This config indicates how to build the final prompt that the LLM reads.\n\n- **name**: Just a name for this prompt.\n- **system_prefix**: This system prompt indicates the LLM how to behave.\n- **system_tag**: The tag to indicate the start of the system prefix for the LLM.\n- **system_end**: The tag to indicate the end of the system prefix for the LLM.\n- **user_tag**: The tag to indicate the start of the user input.\n- **user_end**: The tag to indicate the end of the user input.\n- **assistant_tag**: The tag to indicate the start of the assistant output.\n- **assistant_end**: The tag to indicate the end of the assistant output.\n- **n_contexts_to_use**: The number of contexts from the Retriever to use in the generation process. Default: 3\n\nWe recommend using the [ChatML guidelines](https://github.com/openai/openai-python/blob/main/chatml.md), an of course using LLMs trained using this format, like Llama-2. The example here is the prompt format used by [Llama-2](https://huggingface.co/blog/llama2#how-to-prompt-llama-2):\n\nIn the system prefix you can use the following text behavior:\n```\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n```\n\nThen for the tags it is recommended to use the following:\n```json\n{\n    \"name\": \"Llama2_PromptConfig\",\n    \"system_prefix\": \"You are a helpful, respectful and honest assistant. <Rest of the text> If you don't know the answer to a question, please don't share false information.\\n\\n\",\n    \"system_tag\": \"<s>[INST] <<SYS>>\\n\",\n    \"system_end\": \"\\n<</SYS>>\\n\",\n    \"user_tag\": \"\",\n    \"user_end\": \"[/INST]\\n\",\n    \"assistant_tag\": \"\",\n    \"assistant_end\": \"[/INST]\\n\"\n}\n```\n\nWhen constructing the prompt we just concatenate the tags in the following order:\n1. system_tag\n2. system_prefix\n3. system_end\n4. user_tag\n5. user message\n6. user_end\n7. assistant_tag\n8. assistant output\n9. assistant_end\n\nFor **n_contexts_to_use** a standard practice is to use 3.\n\n> ⚠️ If you use an OpenAI model you only need to specify the **system prefix**, the other fields are not used.\n", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#prompt-config", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 89, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > generation-config", "content": "modules > ia-configuration > the-rag-pipeline > generation-config: The generation config is used to define the characteristics of the second part from the RAG pipeline, the generation process. We use sampling to generate the answer.\n\nThe sampling generation process is configured with the following properties:\n\n- **name**: Just a name for the generation process.\n- **top_k**: The number of tokens to consider for the top-k sampling. Default: 50.\n- **top_p**: The cumulative probability for the top-p sampling. Default: 1.0.\n- **temperature**: The temperature for the sampling. Default: 0.2.\n- **repetition_penalty**: The repetition penalty for the sampling. Default: 1.0.\n- **seed**: The seed for the sampling. Default: 42.\n- **max_new_tokens**: The maximum number of new tokens to generate. Default: 256.\n\nWe recommend setting the temperature to low values, less than 1.0 because we want the model to be factual, not creative. A very good guide of all this parameters can be found in the [HuggingFace documentation](https://huggingface.co/blog/how-to-generate).\n\nAn example of a generation config is the following:\n```json\n{\n    \"name\": \"Llama2_GenerationConfig\",\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"temperature\": 0.2,\n    \"repetition_penalty\": 1.0,\n    \"seed\": 42,\n    \"max_new_tokens\": 256\n}\n```", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#generation-config", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 90, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > the-rag-pipeline > rag-config", "content": "modules > ia-configuration > the-rag-pipeline > rag-config: Finally, the RAG config is used to glue all the previous components together.\n\nIt relates the different elements to create a RAG (Retrieval Augmented Generation) pipeline.\n\nThe RAG config is configured with the following properties:\n\n- name: Just a name for the RAG config.\n- knowledge_base: The knowledge base to use.\n- llm_config: The LLM config to use.\n- prompt_config: The prompt config to use.\n- generation_config: The generation config to use.\n- retriever_config: The retriever config to use.\n\nRemember that currently all the relevant data/models can be accessed and modified from the Django admin panel ([http://localhost/back/admin/](http://localhost/back/admin/)) or from the CLI.\n\n\nAn example of a RAG config is the following:\n```json\n{\n    \"name\": \"chatfaq_llama_rag\",\n    \"knowledge_base\": \"It_KB\",\n    \"llm_config\": \"Llama2_GPU\",\n    \"prompt_config\": \"Llama2_PromptConfig\",\n    \"generation_config\": \"Llama2_GenerationConfig\",\n    \"retriever_config\": \"e5-retriever\"\n}\n```", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#rag-config", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 91, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > ia-configuration > using-your-rag-pipeline", "content": "modules > ia-configuration > using-your-rag-pipeline: To create the RAG pipeline you just need to link all the components together. You can do it from the Django admin panel ([http://localhost/back/admin/](http://localhost/back/admin/)).\n\nThen, if you go to the Celery logs you will see that the RAG pipeline is being built. This process can take several minutes, depending on the size of the knowledge base. When it is finished you will see a message like this:\n```\n[2023-10-20 11:03:22,743: INFO/MainProcess] Loading RAG config: chatfaq_llama_rag with llm: meta-llama/Llama-2-7b-chat-hf with llm type: Local GPU Model with knowledge base: chatfaq retriever: intfloat/e5-small-v2 and retriever device: cpu\n```\n\n\nOnce you have created your RAG pipeline, you can use it to generate answers.\n\nThe last step will be to reference the name of the Rag Config from a state of your SDK's FSM. <a href=\"/en/latest/modules/sdk/index.html#model-example\">Here is an example of that</a>", "url": "https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#using-your-rag-pipeline", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 92, "fields": {"created_date": "2024-04-19T13:19:10.769", "updated_date": "2024-04-19T13:19:10.769", "knowledge_base": 1, "title": "modules > interfaces > init", "content": "modules > interfaces > init: It provides up to 3 interfaces to interact with the backend's models/data:\n\n- [It CLI](./cli/index.md)\n- [It Admin](./django-admin/index.md)\n- [Django Admin](./chatfaq-admin/index.md)", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/index.html", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 93, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > cli > init", "content": "modules > interfaces > cli > init: It Command Line Interface (CLI) Tool\n\nThe CLI allows you to interact with the backend server from the command line. It offers the same capabilities as the Django admin panel or the It admin.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 94, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > cli > prerequisites", "content": "modules > interfaces > cli > prerequisites: Make sure the next list of packages are installed on your system:\n\n- Python 3.10\n- poetry/pip", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#prerequisites", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 95, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > cli > installation > local-build", "content": "modules > interfaces > cli > installation > local-build: `poetry install`", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#local-build", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 96, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > cli > installation > pypi", "content": "modules > interfaces > cli > installation > pypi: `pip install chatfaq-cli`", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#pypi", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 97, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > cli > usage", "content": "modules > interfaces > cli > usage: First of all you should configure the remote target server:\n\n`chatfaq config host <REMOTE_ADRESS>`\n\nThen you can log in into the remote back-end server:\n\n`chatfaq config auth <TOKEN>`\n\nYou can retrieve the auth token from the backend server:\n\n`curl -X POST -u username:password http://localhost:8000/back/api/login/`\n\nby using an admin's user and password.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#usage", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 98, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > cli > commands", "content": "modules > interfaces > cli > commands: For a full list of commands and options run:\n\n`chatfaq --help`", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#commands", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 99, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > cli > notes", "content": "modules > interfaces > cli > notes: For the autocompletion to work you should run:\n\n`chatfaq --install-completion`\n\nor run `chatfaq --show-completion` and add the output to your shell's configuration file.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#notes", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 100, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > chatfaq-admin > init", "content": "modules > interfaces > chatfaq-admin > init: The It admin will be a node application that allows you to manage your chatbot's relevant data from the browser.\n\nIt is currently under development.", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/chatfaq-admin/index.html", "section": "", "role": "", "page_number": null, "metadata": null}}, {"model": "language_model.knowledgeitem", "pk": 101, "fields": {"created_date": "2024-04-19T13:19:10.770", "updated_date": "2024-04-19T13:19:10.770", "knowledge_base": 1, "title": "modules > interfaces > django-admin > init", "content": "modules > interfaces > django-admin > init: The Django admin is an automatic admin interface that comes with Django. It allows you to manage your chatbot's relevant data from the browser.\n\nYou can access it with an <a href=\"/en/latest/modules/installations/index.html#set-up\">admin account</a> at [http://localhost/back/admin/](http://localhost/back/admin/).", "url": "https://chatfaq.readthedocs.io/en/latest/modules/interfaces/django-admin/index.html", "section": "", "role": "", "page_number": null, "metadata": null}},
{"model": "language_model.ragconfig", "pk": 1, "fields": {"created_date": "2024-04-19T13:21:54.275", "updated_date": "2024-04-19T13:21:54.275", "name": "default", "knowledge_base": 1, "llm_config": 1, "prompt_config": 1, "generation_config": 1, "retriever_config": 1, "disabled": false, "s3_index_path": null, "index_status": "no_index"}}]
